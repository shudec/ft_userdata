import zipfile
import json
import pandas as pd
import numpy as np
from datetime import datetime

# ===== MACHINE LEARNING IMPORTS =====
# These libraries will help us detect non-linear relationships and feature interactions
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import mutual_info_classif

# ===== VISUALIZATION IMPORTS =====
import matplotlib.pyplot as plt
import seaborn as sns

# ===== ADVANCED INTERPRETABILITY (OPTIONAL) =====
# SHAP helps us understand how features interact and contribute to predictions
try:
    import shap
    SHAP_AVAILABLE = True
    print("[âœ“] SHAP library available for model interpretability")
except ImportError:
    SHAP_AVAILABLE = False
    print("[!] SHAP not available. Install with: pip install shap")

# Set plotting style for better visualizations
plt.style.use('default')
sns.set_palette("husl")

# === FILE PATHS ===
indicator_file = "user_data/analysis/ichimoku_rebond_entries.csv"
merged_output = "user_data/analysis/merged_trades.csv"
corr_output = "user_data/analysis/indicator_loss_correlation.csv"

# === GET LATEST BACKTEST ZIP FILE ===
last_result_path = "user_data/backtest_results/.last_result.json"
with open(last_result_path, 'r') as f:
    last_result = json.load(f)
    
zip_filename = last_result["latest_backtest"]
zip_path = f"user_data/backtest_results/{zip_filename}"

# Extract the base name without extension for the trades file path
base_name = zip_filename.replace('.zip', '')
trades_file_path = f"{base_name}.json"

print(f"[+] Using backtest file: {zip_path}")
print(f"[+] Looking for trades file: {trades_file_path}")

# === LOAD INDICATOR ENTRIES ===
indicators_df = pd.read_csv(indicator_file, index_col=0)  # Use first column as index
indicators_df["date"] = pd.to_datetime(indicators_df["timestamp"])

# === LOAD TRADES.JSON FROM ZIP ===
with zipfile.ZipFile(zip_path, 'r') as z:
    with z.open(trades_file_path) as f:
        trades_data = json.load(f)

# Extract the actual trades list from the data structure
trades_list = None
if isinstance(trades_data, dict):
    # Check strategy section first (this is where the actual trades list is located)
    if 'strategy' in trades_data and isinstance(trades_data['strategy'], dict):
        strategy_data = trades_data['strategy']
        # Find the first strategy that has trades
        for strategy_name, strategy_content in strategy_data.items():
            if isinstance(strategy_content, dict) and 'trades' in strategy_content:
                trades_list = strategy_content['trades']
                if isinstance(trades_list, list):  # Make sure it's a list, not just a count
                    print(f"[+] Found {len(trades_list)} trades in strategy '{strategy_name}'")
                    break
    
    # Fallback to other locations
    if trades_list is None and 'trades' in trades_data:
        trades_list = trades_data['trades']

if trades_list is None or not isinstance(trades_list, list):
    raise ValueError("Could not find trades data (as a list) in the loaded JSON structure")

trades_df = pd.DataFrame(trades_list)
trades_df.to_csv("user_data/analysis/all_trades.csv", index=False)

# Ensure correct datetime format and timezone consistency
trades_df["open_date"] = pd.to_datetime(trades_df["open_date"]).dt.tz_localize(None)  # Remove timezone
indicators_df["timestamp"] = pd.to_datetime(indicators_df["timestamp"]).dt.tz_localize(None)  # Remove timezone

# Ensure indicators date is also timezone-naive (if it has timezone info)
if indicators_df["date"].dt.tz is not None:
    indicators_df["date"] = indicators_df["date"].dt.tz_localize(None)

# === MERGE ===
merged_df = pd.merge(
    trades_df,
    indicators_df,
    left_on=["pair", "open_date"],
    right_on=["pair", "timestamp"],
    how="inner"
)

# Transform all boolean columns to int
bool_cols = merged_df.select_dtypes(include=["bool"]).columns
merged_df[bool_cols] = merged_df[bool_cols].astype(int)

# === ADD LOSS FLAG ===
merged_df["is_loss"] = (merged_df["profit_abs"] < 0).astype(int)

# === SAVE MERGED DATASET ===
merged_df.to_csv(merged_output, index=False)
print(f"[+] Merged dataset saved to {merged_output}")

# === CORRELATION ANALYSIS ===
# Select only numeric columns
numeric_cols = merged_df.select_dtypes(include=["number"]).columns.tolist()

# Exclude target itself
if "is_loss" in numeric_cols:
    numeric_cols = ["rsi", "atr_strength", "adx", "close_sup_sma200", "ichimoku-chiku-free", "kinjun_flat", "ichimoku-tenkan_sup_kinjun", "tenkan_kinjun_increasing", "kinjun_proximity", "tenkan_proximity", "proximity", "volume_sup_avg"]

correlations = {}
for col in numeric_cols:
    corr = merged_df["is_loss"].corr(merged_df[col])
    correlations[col] = corr

# Convert to DataFrame
corr_df = pd.DataFrame(list(correlations.items()), columns=["indicator", "correlation_with_loss"])
corr_df = corr_df.sort_values(by="correlation_with_loss", ascending=False)

# Save correlations
corr_df.to_csv(corr_output, index=False)
print(f"[+] Correlations saved to {corr_output}")

# Preview
print("\nTop correlated indicators with losing trades:")
print(corr_df.head(10))

# ===== ADVANCED MACHINE LEARNING ANALYSIS =====
print("\n" + "="*80)
print("STARTING ADVANCED ML ANALYSIS FOR NON-LINEAR PATTERNS")
print("="*80)

def create_interaction_features(df, indicator_cols):
    """
    Create polynomial and interaction features to capture non-linear relationships.
    
    This function helps ML models detect:
    1. Polynomial relationships (squared terms) - e.g., RSI might work differently at extremes
    2. Feature interactions - e.g., Volume + RSI together might be more predictive
    
    Args:
        df: DataFrame with indicator columns
        indicator_cols: List of indicator column names
        
    Returns:
        DataFrame with original + engineered features
    """
    print(f"[+] Creating interaction features from {len(indicator_cols)} base indicators...")
    
    # Start with original features
    new_df = df[indicator_cols].copy()
    
    # Add polynomial features (squares) - captures non-linear relationships
    print("    - Adding polynomial features (squared terms)...")
    for col in indicator_cols:
        if col in df.columns:  # Make sure column exists
            new_df[f"{col}_squared"] = df[col] ** 2
    
    # Add two-way interactions - captures how indicators work together
    print("    - Adding two-way interaction features...")
    interaction_count = 0
    for i, col1 in enumerate(indicator_cols):
        for col2 in indicator_cols[i+1:]:
            if col1 in df.columns and col2 in df.columns:
                new_df[f"{col1}_x_{col2}"] = df[col1] * df[col2]
                interaction_count += 1
    
    print(f"    - Created {interaction_count} interaction features")
    print(f"    - Total features: {new_df.shape[1]} (was {len(indicator_cols)})")
    
    return new_df

def analyze_with_random_forest(X_train, X_test, y_train, y_test, feature_names):
    """
    Random Forest Analysis - Excellent for:
    1. Handling non-linear relationships automatically
    2. Feature importance ranking
    3. Robust to outliers
    4. Handles feature interactions naturally
    """
    print("\n[+] Training Random Forest model...")
    
    # Train Random Forest with balanced classes (important for trading data)
    rf_model = RandomForestClassifier(
        n_estimators=100,           # 100 decision trees
        max_depth=10,              # Prevent overfitting
        random_state=42,           # Reproducible results
        class_weight='balanced',   # Handle imbalanced win/loss data
        n_jobs=-1                  # Use all CPU cores
    )
    
    rf_model.fit(X_train, y_train)
    
    # Calculate performance metrics
    y_pred = rf_model.predict(X_test)
    y_pred_proba = rf_model.predict_proba(X_test)[:, 1]
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    print(f"    Random Forest AUC Score: {auc_score:.4f}")
    print(f"    (AUC > 0.7 = Good, > 0.8 = Excellent, 0.5 = Random)")
    
    # Feature importance analysis
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return rf_model, feature_importance, auc_score, y_pred, y_pred_proba

def analyze_with_gradient_boosting(X_train, X_test, y_train, y_test, feature_names):
    """
    Gradient Boosting Analysis - Excellent for:
    1. Sequential learning (learns from previous mistakes)
    2. Handling complex non-linear patterns
    3. Feature importance with interactions
    4. Often outperforms Random Forest
    """
    print("\n[+] Training Gradient Boosting model...")
    
    gb_model = GradientBoostingClassifier(
        n_estimators=100,        # Sequential boosting rounds
        learning_rate=0.1,       # How much each tree contributes
        max_depth=6,             # Tree depth for complexity
        random_state=42,         # Reproducible results
        subsample=0.8            # Use 80% of data per tree (prevents overfitting)
    )
    
    gb_model.fit(X_train, y_train)
    
    # Performance metrics
    y_pred = gb_model.predict(X_test)
    y_pred_proba = gb_model.predict_proba(X_test)[:, 1]
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    print(f"    Gradient Boosting AUC Score: {auc_score:.4f}")
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': gb_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return gb_model, feature_importance, auc_score, y_pred, y_pred_proba

def analyze_with_neural_network(X_train, X_test, y_train, y_test):
    """
    Neural Network Analysis - Excellent for:
    1. Very complex non-linear patterns
    2. Deep feature interactions
    3. Automatic feature learning
    4. Can discover hidden patterns
    """
    print("\n[+] Training Neural Network model...")
    
    # Neural networks need scaled features (all features on similar scales)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Multi-layer perceptron with 3 hidden layers
    nn_model = MLPClassifier(
        hidden_layer_sizes=(64, 32, 16),  # 64->32->16 neurons (funnel shape)
        activation='relu',                 # ReLU activation (good for most problems)
        solver='adam',                     # Adam optimizer (adaptive learning)
        max_iter=500,                      # Maximum training iterations
        random_state=42,                   # Reproducible results
        early_stopping=True,               # Stop if no improvement
        validation_fraction=0.2            # Use 20% for validation
    )
    
    nn_model.fit(X_train_scaled, y_train)
    
    # Performance metrics
    y_pred_proba = nn_model.predict_proba(X_test_scaled)[:, 1]
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    print(f"    Neural Network AUC Score: {auc_score:.4f}")
    print(f"    Training iterations: {nn_model.n_iter_}")
    
    return nn_model, scaler, auc_score, y_pred_proba

def calculate_mutual_information(X, y, feature_names):
    """
    Mutual Information Analysis - Measures ANY kind of dependency:
    1. Linear relationships (like correlation)
    2. Non-linear relationships (unlike correlation)
    3. Complex interactions
    4. Information content between features and target
    """
    print("\n[+] Calculating Mutual Information scores...")
    print("    (This measures ANY kind of relationship, not just linear)")
    
    # Calculate mutual information scores
    mi_scores = mutual_info_classif(X, y, random_state=42)
    
    mi_df = pd.DataFrame({
        'feature': feature_names,
        'mutual_info': mi_scores
    }).sort_values('mutual_info', ascending=False)
    
    print(f"    Calculated MI scores for {len(feature_names)} features")
    
    return mi_df

def generate_shap_analysis(model, X_test, feature_names, max_samples=1000):
    """
    SHAP Analysis - Explains model predictions:
    1. How much each feature contributes to each prediction
    2. Feature interactions
    3. Global vs local importance
    4. Positive/negative contributions
    """
    if not SHAP_AVAILABLE:
        print("[!] SHAP not available - skipping interpretability analysis")
        return None
    
    print(f"\n[+] Generating SHAP explanations (using {max_samples} samples)...")
    
    try:
        # Create SHAP explainer for tree-based models
        explainer = shap.TreeExplainer(model)
        
        # Calculate SHAP values for a subset of test data (for speed)
        sample_size = min(max_samples, len(X_test))
        X_sample = X_test.iloc[:sample_size]
        shap_values = explainer.shap_values(X_sample)
        
        # Create summary plot
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)
        plt.title("SHAP Feature Importance Summary\n(Shows positive/negative impact on loss prediction)")
        plt.tight_layout()
        plt.savefig("user_data/analysis/shap_summary.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print("    SHAP summary plot saved to user_data/analysis/shap_summary.png")
        
        # Calculate mean absolute SHAP values for feature ranking
        mean_shap = np.abs(shap_values).mean(0)
        shap_importance = pd.DataFrame({
            'feature': feature_names,
            'shap_importance': mean_shap
        }).sort_values('shap_importance', ascending=False)
        
        return shap_importance
        
    except Exception as e:
        print(f"[!] SHAP analysis failed: {e}")
        return None

def create_analysis_report(results_dict, output_path="user_data/analysis/ml_analysis_report.md"):
    """
    Generate a comprehensive markdown report of all ML analysis results.
    This creates a professional report explaining findings and recommendations.
    """
    print(f"\n[+] Generating comprehensive analysis report...")
    
    report = f"""# Advanced Machine Learning Analysis Report
*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*

## Executive Summary
This report analyzes trading indicators using advanced machine learning techniques to discover non-linear relationships and feature interactions that simple correlation analysis cannot detect.

## Dataset Overview
- **Total Features Analyzed**: {results_dict['total_features']}
- **Original Indicators**: {results_dict['original_indicators']}
- **Engineered Features**: {results_dict['total_features'] - results_dict['original_indicators']}
- **Training Samples**: {results_dict['train_samples']}
- **Test Samples**: {results_dict['test_samples']}
- **Loss Rate**: {results_dict['loss_rate']:.2%}

## Model Performance Comparison

| Model | AUC Score | Interpretation |
|-------|-----------|----------------|
| Random Forest | {results_dict['rf_auc']:.4f} | {'ðŸŸ¢ Excellent' if results_dict['rf_auc'] > 0.8 else 'ðŸŸ¡ Good' if results_dict['rf_auc'] > 0.7 else 'ðŸŸ  Fair' if results_dict['rf_auc'] > 0.6 else 'ðŸ”´ Poor'} |
| Gradient Boosting | {results_dict['gb_auc']:.4f} | {'ðŸŸ¢ Excellent' if results_dict['gb_auc'] > 0.8 else 'ðŸŸ¡ Good' if results_dict['gb_auc'] > 0.7 else 'ðŸŸ  Fair' if results_dict['gb_auc'] > 0.6 else 'ðŸ”´ Poor'} |
| Neural Network | {results_dict['nn_auc']:.4f} | {'ðŸŸ¢ Excellent' if results_dict['nn_auc'] > 0.8 else 'ðŸŸ¡ Good' if results_dict['nn_auc'] > 0.7 else 'ðŸŸ  Fair' if results_dict['nn_auc'] > 0.6 else 'ðŸ”´ Poor'} |

**AUC Score Guide**: 0.5 = Random, 0.6-0.7 = Fair, 0.7-0.8 = Good, 0.8+ = Excellent

## Top Features by Analysis Method

### Random Forest Feature Importance
*(Measures feature importance in tree-based decisions)*
"""
    
    # Add top features from each method
    for i, row in enumerate(results_dict['rf_importance'].head(10).itertuples()):
        report += f"\n{i+1}. **{row.feature}**: {row.importance:.4f}"
    
    report += f"""

### Gradient Boosting Feature Importance
*(Measures sequential learning importance)*
"""
    
    for i, row in enumerate(results_dict['gb_importance'].head(10).itertuples()):
        report += f"\n{i+1}. **{row.feature}**: {row.importance:.4f}"
    
    report += f"""

### Mutual Information Scores
*(Measures ANY kind of relationship - linear or non-linear)*
"""
    
    for i, row in enumerate(results_dict['mi_scores'].head(10).itertuples()):
        report += f"\n{i+1}. **{row.feature}**: {row.mutual_info:.4f}"
    
    # Add SHAP results if available
    if results_dict.get('shap_importance') is not None:
        report += f"""

### SHAP Feature Importance
*(Explains individual prediction contributions)*
"""
        for i, row in enumerate(results_dict['shap_importance'].head(10).itertuples()):
            report += f"\n{i+1}. **{row.feature}**: {row.shap_importance:.4f}"
    
    report += f"""

## Key Insights and Recommendations

### Performance Analysis
- **Best Performing Model**: {results_dict['best_model']} (AUC: {results_dict['best_auc']:.4f})
- **Model Comparison**: {'Models show significant improvement over random baseline' if results_dict['best_auc'] > 0.6 else 'Models struggle to predict losses effectively'}

### Feature Engineering Impact
- **Original vs Enhanced**: Enhanced feature set with interactions {'significantly improved' if results_dict['best_auc'] > 0.65 else 'moderately improved'} prediction accuracy
- **Most Valuable Features**: {', '.join(results_dict['top_features'][:3])}

### Trading Strategy Implications
1. **Feature Combinations**: Look for combinations of indicators rather than single indicators
2. **Non-Linear Thresholds**: Consider that indicators may work differently at extreme values
3. **Volume Integration**: Volume-based features show {'high' if 'volume' in str(results_dict['top_features'][:5]).lower() else 'moderate'} importance

### Next Steps
1. **Strategy Integration**: Implement top-performing features in your trading strategy
2. **Threshold Optimization**: Use model predictions to set entry/exit thresholds
3. **Continuous Monitoring**: Retrain models periodically as market conditions change
4. **Feature Selection**: Focus on top 10-15 features to avoid overfitting

## Technical Notes
- **Cross-Validation**: Used stratified train-test split to maintain class balance
- **Feature Scaling**: Applied for neural network analysis
- **Overfitting Prevention**: Used regularization and early stopping
- **Class Imbalance**: Handled using balanced class weights where applicable

---
*This analysis was generated using scikit-learn machine learning algorithms with feature engineering for interaction detection.*
"""
    
    # Save report
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"    Comprehensive report saved to {output_path}")
    return report

# ===== EXECUTE MACHINE LEARNING ANALYSIS =====

# Define indicator columns to analyze
indicator_cols = [
    'rsi', 'atr_strength', 'adx', 'close_sup_sma200', 
    'ichimoku-chiku-free', 'kinjun_flat', 'ichimoku-tenkan_sup_kinjun',
    'tenkan_kinjun_increasing', 'kinjun_proximity', 'tenkan_proximity', 
    'proximity', 'volume_sup_avg'
]

# Filter to only include indicators that actually exist in the dataset
existing_indicators = [col for col in indicator_cols if col in merged_df.columns]
missing_indicators = [col for col in indicator_cols if col not in merged_df.columns]

if missing_indicators:
    print(f"[!] Missing indicators: {missing_indicators}")

print(f"[+] Analyzing {len(existing_indicators)} indicators: {existing_indicators}")

# Create enhanced feature set with interactions
try:
    X_enhanced = create_interaction_features(merged_df, existing_indicators)
    y = merged_df["is_loss"]
    
    print(f"\n[+] Dataset prepared:")
    print(f"    - Original features: {len(existing_indicators)}")
    print(f"    - Enhanced features: {X_enhanced.shape[1]}")
    print(f"    - Total samples: {len(y)}")
    print(f"    - Loss rate: {y.mean():.2%}")
    
    # Split data for training and testing
    X_train, X_test, y_train, y_test = train_test_split(
        X_enhanced, y, 
        test_size=0.2,           # 20% for testing
        random_state=42,         # Reproducible results
        stratify=y               # Maintain loss/win ratio in both sets
    )
    
    print(f"    - Training samples: {len(X_train)}")
    print(f"    - Test samples: {len(X_test)}")
    
    # Store results for reporting
    results = {
        'total_features': X_enhanced.shape[1],
        'original_indicators': len(existing_indicators),
        'train_samples': len(X_train),
        'test_samples': len(X_test),
        'loss_rate': y.mean()
    }
    
    # ===== RANDOM FOREST ANALYSIS =====
    rf_model, rf_importance, rf_auc, rf_pred, rf_pred_proba = analyze_with_random_forest(
        X_train, X_test, y_train, y_test, X_enhanced.columns
    )
    results['rf_auc'] = rf_auc
    results['rf_importance'] = rf_importance
    
    # ===== GRADIENT BOOSTING ANALYSIS =====
    gb_model, gb_importance, gb_auc, gb_pred, gb_pred_proba = analyze_with_gradient_boosting(
        X_train, X_test, y_train, y_test, X_enhanced.columns
    )
    results['gb_auc'] = gb_auc
    results['gb_importance'] = gb_importance
    
    # ===== NEURAL NETWORK ANALYSIS =====
    nn_model, scaler, nn_auc, nn_pred_proba = analyze_with_neural_network(
        X_train, X_test, y_train, y_test
    )
    results['nn_auc'] = nn_auc
    
    # ===== MUTUAL INFORMATION ANALYSIS =====
    mi_scores = calculate_mutual_information(X_enhanced, y, X_enhanced.columns)
    results['mi_scores'] = mi_scores
    
    # ===== SHAP INTERPRETABILITY ANALYSIS =====
    # Use the best performing model for SHAP analysis
    best_auc = max(rf_auc, gb_auc, nn_auc)
    if best_auc == rf_auc:
        best_model_name = "Random Forest"
        shap_model = rf_model
    elif best_auc == gb_auc:
        best_model_name = "Gradient Boosting"
        shap_model = gb_model
    else:
        best_model_name = "Neural Network"
        shap_model = None  # SHAP doesn't work well with neural networks
    
    results['best_model'] = best_model_name
    results['best_auc'] = best_auc
    
    if shap_model is not None:
        shap_importance = generate_shap_analysis(shap_model, X_test, X_enhanced.columns)
        results['shap_importance'] = shap_importance
    
    # ===== SAVE DETAILED RESULTS =====
    print("\n[+] Saving detailed analysis results...")
    
    # Save feature importance results
    rf_importance.to_csv("user_data/analysis/ml_random_forest_importance.csv", index=False)
    gb_importance.to_csv("user_data/analysis/ml_gradient_boosting_importance.csv", index=False)
    mi_scores.to_csv("user_data/analysis/ml_mutual_information.csv", index=False)
    
    if results.get('shap_importance') is not None:
        results['shap_importance'].to_csv("user_data/analysis/ml_shap_importance.csv", index=False)
    
    # Get top features across all methods
    top_rf = rf_importance.head(5)['feature'].tolist()
    top_gb = gb_importance.head(5)['feature'].tolist()
    top_mi = mi_scores.head(5)['feature'].tolist()
    
    # Combine and get unique top features
    all_top_features = list(set(top_rf + top_gb + top_mi))
    results['top_features'] = all_top_features
    
    # ===== CREATE COMPREHENSIVE REPORT =====
    report = create_analysis_report(results)
    
    # ===== SUMMARY =====
    print("\n" + "="*80)
    print("MACHINE LEARNING ANALYSIS COMPLETE!")
    print("="*80)
    print(f"âœ“ Best Model: {best_model_name} (AUC: {best_auc:.4f})")
    print(f"âœ“ Performance: {'Excellent' if best_auc > 0.8 else 'Good' if best_auc > 0.7 else 'Fair' if best_auc > 0.6 else 'Needs Improvement'}")
    print(f"âœ“ Files Generated:")
    print(f"  - ML Analysis Report: user_data/analysis/ml_analysis_report.md")
    print(f"  - Random Forest Results: user_data/analysis/ml_random_forest_importance.csv")
    print(f"  - Gradient Boosting Results: user_data/analysis/ml_gradient_boosting_importance.csv")
    print(f"  - Mutual Information Results: user_data/analysis/ml_mutual_information.csv")
    if SHAP_AVAILABLE and shap_model is not None:
        print(f"  - SHAP Interpretability: user_data/analysis/ml_shap_importance.csv")
        print(f"  - SHAP Visualization: user_data/analysis/shap_summary.png")
    
    print(f"\nðŸŽ¯ TOP PREDICTIVE FEATURES:")
    for i, feature in enumerate(all_top_features[:10], 1):
        print(f"   {i}. {feature}")
    
    print(f"\nðŸ“Š Read the full report at: user_data/analysis/ml_analysis_report.md")
    
except Exception as e:
    print(f"[!] Error in ML analysis: {e}")
    import traceback
    traceback.print_exc()
